---
title: "rts_data_processing"
output: rmarkdown::html_vignette
author: Jannis Gottwald
vignette: >
  %\VignetteIndexEntry{rts_data_processing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

The use of VHF transmitters is still the only option to track the movements of small animals (https://science.sciencemag.org/content/348/6240/aaa2478). However, the method in its manual form is extremely labour-intensive and spatially and temporally poorly resolved. Recent developments allow automation of the recording of VHF signals and thus the spatially and temporally highly resolved observation of even very small animals (https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13294). The data volumes involved are considerable, and efficient data processing is needed. This vignette explains all the steps from the processing of raw data recorded with this [software](https://github.com/Nature40/Sensorboxes-Images/releases/tag/radiotracking-0.4.3) to the triangulations. All necessary functionalities are provided in the [tRackIT R package](https://github.com/Nature40/tRackIT/).  In this Vignette [test data](https://hessenbox.uni-marburg.de/getlink/fiSz2C9chiQUmj5LmLXyxAzm/data) generated by Kim Lindner and Marcel Becker is used.

## Packages needed

```{r setup, message=FALSE, warning=FALSE }
library(tRackIT)
library(plyr)
library(data.tree)
library(raster)
library(mapview)
library(data.table)
library(devtools)
```

```{r eval=FALSE}

install_github("Nature40/tRackIT")
````

## Required data
+ **Stations.csv:** data.frame with Name of the tRackIT station, Coordinates in Lat/Lon and orientation of each antenna of the station. Colnames have to be Name;Longitude;Latitude;receiver;orientation. Should be stored in projroot/data/reference_data/
+ **Calibration curves:** tRackIT data recorded during a [calibration session](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13294). One file per station and must carry complete station name in file name. Data should be stored in projroot/data/calibration_curves/. Only necessary if calibration is desired (recommended).  
+ **tRackIT data:** either raw logger data [test data](https://hessenbox.uni-marburg.de/getlink/fiSz2C9chiQUmj5LmLXyxAzm/data) as it is produced by the [software](https://github.com/Nature40/Sensorboxes-Images/releases/tag/radiotracking-0.4.3) running on the pi, or allready processed tRackIT data. 

## Initialize Project
The package is structured in such a way that a project folder (projroot) is created at the beginning, in which various subfolders for products and reference data are stored. The **initProject()* function creates these folders as well as a R object in the form of a list in which all paths are stored. In addition, the path to absolute raw data, as created by the [software](https://github.com/Nature40/Sensorboxes-Images/releases/tag/radiotracking-0.4.3) running on the stations, which may be stored on an external hard disk and do not need to be integrated into the project , can be specified in the iniProject function. For this tutorial please store the [test data](https://hessenbox.uni-marburg.de/getlink/fiSz2C9chiQUmj5LmLXyxAzm/data) under the path to raw data (logger_data_raw).

```{r eval=FALSE}

plst<-initProject(projroot = "path/project_name/", logger_data_raw = "path/to/raw/data/")
````

The created folder structure looks like this

```{r,results='hide', include=FALSE}
path <- c(
    "projroot/data/batch_awk/", 
    "projroot/data/calibration_curves/", 
    "projroot/data/catalogues/", 
    "proroot/data/correction_values/", 
    "projroot/data/individuals/",
    "projroot/data/kplv/",
    "projroot/data/logger_data_csv/",
    "projroot/data/param_lst/",
    "projroot/data/reference_data/"
    
)


x <- lapply(strsplit(path, "/"), function(z) as.data.frame(t(z)))
x <- rbind.fill(x)
x$pathString <- apply(x, 1, function(x) paste(trimws(na.omit(x)), collapse="/"))
(folder.structure <- data.tree::as.Node(x))
```

```{r}
folder.structure
```

+ **/batch_awk:** executable .bat files for coarse filtering (product of **batch.awk()**) are stored here
+ **/calibration_curves:** tRackIT data recorded during a [calibration session](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13294) is stored here.
+ **/catalogues:** Catalogues listing all raw logger data and applied config parameters (product of **file.catalogue()**) or listing all files in logger_data_csv (product of **data.catalogue()**) are stored here.
+ **/correction_values:** Calibration values for each station (product of **calibrate()**) are stored here.
+ **/Individuals:** For each processed Individual a main directory and several subdirectories are created (product of **initAnimal()**).
+ **/kplv:** Keepallive data indicating the functionality of the receivers can be stored here.
+ **/logger_data_csv:** The raw logger data is merged into one data set per station by **read.logger.data()** and stored here.
+ **/param_lst:** Filter parameters applied in **filter.interactive()** are stored here
+ **/reference_data:** Reference data such as the Station.csv or a data.frame containing information about tagged individuals should be stored here  


## Raw data processing
The following shows how the raw data, as it comes from the software running on the stations in the field, is processed. The data is located under *logger_data_raw* as specified in the **initProject()** function. The folder structure there must look like this:  

```{r,results='hide', include=FALSE}
path <- c(
    "logger_data_raw/NameStation1/collection1/", 
    "logger_data_raw/NameStation1/collection2/",
    "logger_data_raw/NameStation2/collection1/", 
    "logger_data_raw/NameStation2/collection2/",
    "logger_data_raw/NameStation3/collection1/",
    "/.../"
    
    
)


x <- lapply(strsplit(path, "/"), function(z) as.data.frame(t(z)))
x <- rbind.fill(x)
x$pathString <- apply(x, 1, function(x) paste(trimws(na.omit(x)), collapse="/"))
(logger_data_raw <- data.tree::as.Node(x))
```

```{r}
logger_data_raw
```

The name of the station must be exactly the same as in the reference table for the stations (**Station.csv-required data**). Collections can be named according to the date of data collection, for example. The data looks like this: for each start time, a .csv is created for each receiver (e.g. 2020-06-09T173044_0.csv for receiver 0) as well as a configuration file for all receivers containing all the parameters set for the run (2020-06-09T173044.conf).

First, a **file catalogue** is created from the raw data, in which the name of each file and the config parameters are saved. This is done with the help of the function **file.catalogue()**. The function receives the location of the raw data from the *projList* (**initProject()**). The catalogues are created per station and collection and therefore these must be specified.




```{r eval=FALSE}

file.catalogue(projList = plst, station="mof_rts_00009", collection = "duration_test" )
```

The catalogue is stored under *projroot/data/catalogues* with the naming convention *StationName_collection_FROM_yyyy-mm-dd_TO_yyyy-mm-dd_file_catalogue.csv*. 
In order to create a common .csv file from the many individual files per station, receiver and start time, the **read.logger.data()** function is used. Data.tables per station are stored in *projroot/data/logger_data_csv/* with the naming convention *StationName_FROM_yyyy-mm-dd_TO_yyyy-mm-dd.csv*.

```{r eval=FALSE}
read.logger.data(projList = plst, station="mof_rts_00009",collection = "duration_test" )

```

finally a **data catalogue** (**data.catalogue()**) of all station files is generated. It contains the name of the file, the start and end date of the recording. If the raw data has already been processed in a former project or is stored outside the projroot folder for any other reason, this can be specified in the function
```{r eval=FALSE}
data.catalogue(projList = plst,in_project = TRUE)

#if in_project=FALSE
#data.catalogue(projList = plst,in_project = FALSE, path_to_folder = "path/to/processed/data/")

```


## Individual processing

Processing of individuals involves

+ Filtering (coarse, fine, collisions)
+ Timematching of receivers per station
+ Bearing calculation
+ Timematching of stations
+ Triangulations

The **initAnimal()** function is used to create a folder structure under *projroot/data/individuals/* for the individual to be processed, as well as an R object with two lists. One list contains the meta information such as *ID*, *frequency*, *start* and *end* time, and the other the paths to the newly created folders.


```{r eval=FALSE}
anml<-initAnimal(projList = plst, freq = 150007, start = "2020-09-23", end="2020-10-02", animalID = "150007_10ms")

```

```{r,results='hide', include=FALSE}
path <- c(
    
    "projroot/data/individuals/IndividualA/bearings/",
    "projroot/data/individuals/IndividualA/calibrated/",
    "projroot/data/individuals/IndividualA/filtered/",
    "projroot/data/individuals/IndividualA/filtered_awk/",
    "projroot/data/individuals/IndividualA/gps_timematch/",
    "projroot/data/individuals/IndividualA/imputed/",
    "projroot/data/individuals/IndividualA/logger_timematch/",
    "projroot/data/individuals/IndividualA/station_timematch/",
    "projroot/data/individuals/IndividualA/triangulations/",
    "projroot/data/individuals/IndividualB/bearings/",
    "projroot/data/individuals/IndividualB/calibrated/",
    "projroot/data/individuals/IndividualB/filtered/",
    "projroot/data/individuals/IndividualB/filtered_awk/",
    "projroot/data/individuals/IndividualB/gps_timematch/",
    "projroot/data/individuals/IndividualB/imputed/",
    "projroot/data/individuals/IndividualB/logger_timematch/",
    "projroot/data/individuals/IndividualB/station_timematch/",
    "projroot/data/individuals/IndividualB/triangulations/"
    
    
)


x <- lapply(strsplit(path, "/"), function(z) as.data.frame(t(z)))
x <- rbind.fill(x)
x$pathString <- apply(x, 1, function(x) paste(trimws(na.omit(x)), collapse="/"))
(folder.structure <- data.tree::as.Node(x))
```

```{r}
folder.structure
```

+ **/bearings:** calculated bearings as product of **calc_bearings()**.
+ **/calibrated:** Calibrated data
+ **/filtered:** Fine filtered data as a product of **filter.interactive()**
+ **/filtered_awk:** Coarse filtered data using awk (**batch.awk()**).
+ **/gps_timematch:** If gps matched reference data is created, store it here.
+ **/imputed:** If missing data is handled, products should be stored here
+ **/logger_timematch:** The time stamps of the same signal can differ on the different receivers of the same station (problem in fft). The function **time_match_logger()** deals with the problem. Results are stored here.
+ **/station_timematch:** For triangulation, stations need to be timematched. Results of **time_match_station()** are stored here.
+ **/triangulations:** Triangulations as results of **triangulate()** are stored here




### Filter
The data is filtered in two steps. In a rough filtering step, the programming language awk is used to create a file roughly filtered by time period, frequency and signal length. Subsequently, the fine filtering is carried out with the help of a semi-automated interactive function.


### Coarse filter
It's time to experience the magic of awk. This may require an installation process. For a short tutorial under Windows see [here](https://stackoverflow.com/questions/21927944/how-to-run-an-awk-commands-in-windows). 

The **batch.awk()** function creates an awk command for the respective individual in which *start* and *end* time, *frequency*, the allowed *frequency error* (with high signal strengths there is a scattering of the frequency of up to 15 kHz), the minimum and maximum expected *signal length* must be specified and the list generated by the **initAnimal()** function must be provided. Again, it must be decided wether the data to be processed ist within the project (projroot/data/logger_data_csv) or somewehere else. The **batch.awk()** function makes youse of the *data.calogue* to only include files that contain data within the start and end date of the individual. It  creates an executable .bat file for each individual containig the awk command. Files are stored here: *projroot/data/batch_awk*. 
Because awk works very efficiently, many .bat files can be executed (by double click) at the same time without overloading the memory.

```{r eval=FALSE}
 batch.awk(animal=anml, projList = plst, freq_er_l = 10, freq_er_r = 10, min_dur = 0.0096, max_dur = 0.0128, in_project = TRUE)

#if in_project=FALSE
#batch.awk(animal=anml, projList = plst, freq_er_l = 10, freq_er_r = 10, min_dur = 0.0096, max_dur = 0.0128,in_project = FALSE, path_to_folder = "path/to/processed/data/")

```


### Fine filter

For fine filtering there is an interactive function (**filter.interactive()**). It needs the project list (**initProject()**), the individual list (**initAnimal()**), the path to a awk filtered file and a number to indicate how the dataset should be subset for the plots. The number should be higher if a lot of data is to be filtered.

```{r eval=FALSE}
filter_interactive(projList=plst,animal = anml,path_to_data ="projroot/data/individuals/150007_10ms/filtered_awk/mof_rts_00009_FROM_2020-09-22_TO_2020-10-01.csv", nth=100)

```

The first thing the function wants to know is whether to continue.

* continue?  
You can answer the question with yes (Y) or no (N). No should only be selected if the plotted data suggests that it is only noise.

Next you are asked whether you want to cut off the data in the upper dB range because, for example, another transmitter is transmitting into it?

* upper cut  
If not, enter a value that lies above your distribution. e.g. 110  

Next, you can cut off the data below a value to the right and left of the centre frequency. Here you first roughly cut out the entire mushroom-shaped data distribution - e.g. <80dB left (-3) right (+10).

* below the signal strength value the frequnency filter is applied:  
80
* max accepted freq error left of signal frequency (negativ value):  
-3
* max accepted freq error right of signal frequency (positive value):  
3

The data distribution after filtering is plotted and you are asked if you are satisfied with it. You can answer with either yes (Y) or no (N). If your answer is no, the last filtering step will be repeatet.

* success?:  
Y  

Next, a finer cutting is performed below the umbrella-shaped distribution in the upper dB range. e.g. below 60dB left(-0.5) and right (1):  

* below the signal strength value the frequnency filter is applied:  
60
* max accepted freq error left of signal frequency (negativ value):  
-0.5
* max accepted freq error right of signal frequency (positive value):  
1  

Again, the data distribution after filtering is plotted and you are asked if you are satisfied with it.

* success?:  
Y  

In a final filtering step, the actual mean frequency of the distribution is calculated, as well as four times the standard deviation, and these values are finally used to filter the lowest dB range, where there is usually a lot of noise. First you need to specify a lower dB value and an upper dB value. The Data between the two values is used for parameter calculatrion. Let´s use 52 and 60 dB.  

* lower max signal limit for parameter calculation:  
52
* upper max signal limit for parameter calculation:  
60  

You will be asked if the calculated parameters fit. If the standard deviation is <1 and the mid frequency matches the given frequency of the transmitter answer with Y

* Mid frequency: 150007.348517199; sd: 0.55782562459287 okay? Y/N:  
Y 

Finally, you will be asked at which dB value the parameters should be applied. Take a dB value just above the noise floor.
* upper cut  
52  


The distribution is plotted and you are asked if the filtering was successful. If you now answer with Y, the filtered file is saved under projroot/data/individuals/individual/filtered/file.csv.

* success?:  
Y


This normally completes the filtering. If several transmitters with different durations have been running on the same frequency, the [collision filter](https://github.com/Nature40/tRackIT/blob/main/vignettes/colision_detection.Rmd) must be applied here.

### Timematch receivers of the same station

Before bearing calculation the filtered data (*data/individuals/filtered*) needs to be timematched by individual Antennas of the station. To apply the time_match_logger() function, the individual list (**initAnimal()**) and the path to the data need to be specified. If [collision fitered data](https://github.com/Nature40/tRackIT/blob/main/vignettes/colision_detection.Rmd) is used, this can be indicated by the *collision=* argument. If *collision=TRUE* a suffix is appended to the resulting filename. The resulting files are stored under *data/individuals/individual/logger_timematch*.


```{r eval=FALSE}
 time_match_logger(animal=anml, path_to_data = "data/individuals/150007_10ms/filtered/mof_rts_00009_FROM_2020-09-22_TO_2020-10-01_filtered", collision = TRUE)
```


### Bearing calculation

Bearings (**calc_bearings()**) are calculated based on the principle of bearing calculation described [here](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13294). Both, the acos and the linear approach are calculated. The following arguments need to be provided:  

* projList: result of **initProject()**  
* animal: result of **initAnimal()**
* path_to_data: path to the **time_match_logger()** data of a station.
* antennas: **Station.csv**- see required data
* dbLoss: expected gain loss in dB between 0° and 90°. Antennaspecific- With HB9Cv antennas best results are obtained with 21dB
* calibrate: if *calibrate=TRUE* the function searches for correction values in *data/correction_values* and calibrates the data  

Results are stored in *data/individuals/individual/bearings*.


```{r eval=FALSE}

# Station.csv named antennas_2020.csv is sored in data/reference/data
ant<- ant<-read.csv(paste0(plst$path$ref, "antennas_2020.csv"))
calc_bearings(projList=plst, animal=anml, path_to_data = "data/indivisuals/150007_10ms/mof_rts_00009_FROM_2020-09-22_TO_2020-10-01_logger_time_match.csv", antennas =ant , dbLoss = 21, calibrate=TRUE )


````


### Time match stations

For triangulations the signals received on different stations need to be timematched. This step is applied when all relevant stations are filtered(**batch.awk()**, **filter_interactive()**), loggers of stations are timematched (**time_match_logger()**) and bearings are calculated (**calc_bearings()**) . The function **time_match_stations()** lists all data stored in *data/individuals/individual/bearings*. Therefore the individual list (**initAnimal()**) must be provided. If *collision=TRUE* only files with the suffix collision are listed. You must decide which bearing calculation method you want to choose (currently only *acos* and *linear*).  One file containig data of all stations is created and stored under */data/individuals/individual/station_timematch/*. The method is appended as a suffix to the file name.

```{r eval=FALSE}

time_match_station(animal=anml, method="linear", collision=TRUE)

````


### Triangulate
 Finally triangulations using all possible dual combinations of stations are calculated (**triangulate()**). Again the bearing methos must be indicated, the path to the data resulting from **time_match_station()** must be provided as well as the individual list (**initAnimal()**) and the Staion.csv. If *collision=TRUE* a suffix is appended to the file name. Dat is stored under *data/individuals/individual/triangulations/*.


```{r eval=FALSE}

triangulate(method = "linear", path_to_data = "data/individuals/15007_10ms/station_timematch/150007_10ms_FROM_2020-09-22_TO_2020-10-01_station_timematch_linear.csv",antennas = ant, animal = anml, collision=TRUE)
````

### Plot triangulations

```{r eval=FALSE}

#get triangulations
fls<-list.files(anml$path$triangulations, full.names = T)
tri<-fread(fls[1])
#delete na coordinates
tri<-tri[!(is.na(tri$X)|is.na(tri$Y)),]
tri<-as.data.frame(tri)
#delete data with to flat or to sharp intersection angles
tri<-tri[tri$intersection<=140 & tri$intersection>=40,]
#only data with a dB value >60 on the main antenna
tri<-tri[tri$max_dB_s1>=60 & tri$max_dB_s2>=60,]
tri<-tri[!(is.na(tri$X)|is.na(tri$Y)),]

#plot relevant antennas
plot_ant<-ant[ant$Name=="mof_rts_00004" | ant$Name=="mof_rts_00009",]
#trabsform to spatial data
coordinates(tri) <- c("X", "Y")
proj4string(tri) <- CRS("+init=epsg:4326") # WGS 84
coordinates(plot_ant) <- c("Longitude", "Latitude")
proj4string(plot_ant) <- CRS("+init=epsg:4326") # WGS 84
#data<- spTransform(data, CRS("+init=epsg:32632"))
mapview(tri, color="red")+plot_ant

````



## Final notes

This package provides a processing frame work for tRackIT data, enabling easy exchange of products and scripts. Automatic processing of multiple individuals can be easily acomplished by e.g. for-looping through a data.frame of individual information. But this is up to you know!



